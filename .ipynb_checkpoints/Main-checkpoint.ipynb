{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db8e4ee",
   "metadata": {},
   "source": [
    "# COGS 181 Neural Networks & Deep Learning Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e337f",
   "metadata": {},
   "source": [
    "Import the necessary packages\n",
    "\n",
    "- Pandas and NumPy for data management\n",
    "- Matplotlib for plotting and displaying images\n",
    "- Torch and its submodules for building the network\n",
    "- Torchvision for doing image processing operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f40ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63dc3a1",
   "metadata": {},
   "source": [
    "Define the device on which we should train the network and store each tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a15c4f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "def display_image(tsr_img):\n",
    "    plt.imshow(tsr_img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb3969",
   "metadata": {},
   "source": [
    "Set each path variable for retrieving the images and their labels.\n",
    "\n",
    "The setup for this training is the following:\n",
    "\n",
    "1. If the path to the directory of classified images exists, clear it for new session\n",
    "2. If it does not exist, create it.\n",
    "3. Copy every file that is already classified from the directory with all images (classified and unclassified) to the directory with classified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "625ddb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = './model_resources/images.csv'\n",
    "img_dir = './model_resources/classified_images'\n",
    "all_images_dir = './classification_interface/images'\n",
    "\n",
    "# remove all existing image copies from img_dir\n",
    "if os.path.exists(img_dir):\n",
    "    os.system(f'Remove-Item {img_dir}/*')\n",
    "else:\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "# copy all classified images into img_dir\n",
    "labels = pd.read_csv(labels_path, header=None)\n",
    "\n",
    "for img_filename in labels[0]:\n",
    "    # copy file from all_images_dir to img_dir\n",
    "    source = os.path.join(all_images_dir, img_filename)\n",
    "    destination = os.path.join(img_dir, img_filename)\n",
    "    shutil.copy(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b46b9dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file, header=None)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx,0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc6fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"classification_interface/images\"\n",
    "\n",
    "# number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# size of batch for training\n",
    "batch_size = 128\n",
    "\n",
    "# input size of all images\n",
    "image_size = 200\n",
    "\n",
    "# number of channels\n",
    "nc = 3\n",
    "\n",
    "# size of Z (latent vector)\n",
    "nz = 100\n",
    "\n",
    "# size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# size of features maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# number of epochs for training\n",
    "num_epochs = 5\n",
    "\n",
    "# learning rate\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# number of GPUs\n",
    "ngpu = 1\n",
    "\n",
    "# ratio of training to testing data\n",
    "training_ratio = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3028bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_seq = transforms.Compose([\n",
    "    transforms.Resize(size=(300, 300), antialias=True),\n",
    "    transforms.CenterCrop(size=(300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f095c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a custom dataset\n",
    "dataset = CustomImageDataset(labels_path, img_dir, transform_seq)\n",
    "\n",
    "# Split up the data into testing and training data\n",
    "dataset_size = dataset.__len__()\n",
    "training_size = math.floor(dataset_size * training_ratio)\n",
    "test_size = dataset_size - training_size\n",
    "\n",
    "training_data, test_data = torch.utils.data.random_split(dataset, [training_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "\n",
    "all_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5394ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edc2e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "756ac9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = Generator(ngpu).to(device)\n",
    "    \n",
    "netG.apply(weights_init)\n",
    "\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a34c20cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9da2fe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "netD.apply(weights_init)\n",
    "\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9717ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718c4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop...\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print('Starting training loop...')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, data in enumerate(all_dataloader, 0):\n",
    "    \n",
    "        # Discriminator\n",
    "    \n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_cup = data[0].to(device)\n",
    "        \n",
    "        b_size = real_cpu.size(0)\n",
    "        \n",
    "        label = torch.full((b.size,), real_label, dtype=torch.float, device=device)\n",
    "        \n",
    "        output = netD(real_cpu).view(-1)\n",
    "        \n",
    "        errD_real = criterion(output, label)\n",
    "        \n",
    "        errD_real.backward()\n",
    "        \n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        \n",
    "        fake = netG(noise)\n",
    "        \n",
    "        label.fill_(fake_label)\n",
    "        \n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        \n",
    "        errD_fake = criterion(output, label)\n",
    "        \n",
    "        errD_fake.backward()\n",
    "        \n",
    "        D_G_z1 = output.mean().item()\n",
    "        \n",
    "        errD = errD_real + errD_fake\n",
    "        \n",
    "        optimizerD.step()\n",
    "        \n",
    "        # Generator\n",
    "        \n",
    "        netG.zero_grad()\n",
    "        \n",
    "        label.fill_(real_label)\n",
    "        \n",
    "        output = netD(fake).view(-1)\n",
    "        \n",
    "        errG = criterion(output, label)\n",
    "        \n",
    "        errG.backward()\n",
    "        \n",
    "        D_G_z2 = output.mean().item()\n",
    "        \n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Stats\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f' % (epoch, num_epochs, i, len(dataloader),errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            \n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08f068",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[[1] Optimal ratio for data splitting](https://onlinelibrary.wiley.com/doi/full/10.1002/sam.11583)\n",
    "\n",
    "[[2] DCGAN Tutorial - PyTorch](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6063a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_data = pd.read_csv('./archive/images.csv')\n",
    "\n",
    "# # clean and format image data\n",
    "# image_data.sort_values(by=['image'], inplace=True)\n",
    "# image_data.reset_index(inplace=True)\n",
    "# image_data.drop(columns=['index', 'sender_id'], inplace=True)\n",
    "\n",
    "# image_data.columns = ['image', 'type', 'kids']\n",
    "\n",
    "# types = image_data['type']\n",
    "\n",
    "# encoded = np.zeros((len(types), len(types.unique())))\n",
    "\n",
    "# for idx in range(len(types)):\n",
    "#     encoded[idx][list(types.unique()).index(types[idx])] = 1\n",
    "    \n",
    "# image_data = pd.concat([image_data, pd.DataFrame(encoded)], axis=1)\n",
    "\n",
    "# image_data.columns = np.concatenate((image_data.columns[:3], types.unique()))\n",
    "\n",
    "# image_data.columns = [c.lower().replace('-', '_').replace(' ', '_') for c in image_data.columns]\n",
    "\n",
    "# image_data.drop(columns=['type'], inplace=True)\n",
    "\n",
    "# image_data['kids'] = image_data['kids'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc44f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
