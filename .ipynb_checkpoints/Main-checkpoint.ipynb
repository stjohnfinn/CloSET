{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db8e4ee",
   "metadata": {},
   "source": [
    "# COGS 181 Neural Networks & Deep Learning Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e337f",
   "metadata": {},
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f40ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a15c4f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "def display_image(tsr_img):\n",
    "    plt.imshow(tsr_img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4518d688",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56aa2713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  360\n"
     ]
    }
   ],
   "source": [
    "# CNN Configuration\n",
    "\n",
    "labels_path = 'data/labels/clothing.csv'\n",
    "img_dir = 'data/data_cnn'\n",
    "all_images_dir = 'classification/images'\n",
    "training_ratio = 0.85\n",
    "\n",
    "# GAN Configuration\n",
    "\n",
    "dataroot = 'data/data_gan'\n",
    "workers = 2\n",
    "batch_size = 128\n",
    "image_size = 64\n",
    "nc = 3\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "num_epochs = 5\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "ngpu = 1\n",
    "\n",
    "# Random seed\n",
    "manualSeed = 360\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cabacf",
   "metadata": {},
   "source": [
    "## Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9b345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_seq = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2e4b3",
   "metadata": {},
   "source": [
    "## Loading CNN Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c2f7a1",
   "metadata": {},
   "source": [
    "### Copy all classified images into `img_dir`\n",
    "\n",
    "Because I have given these images labels by hand, there are a few that have not been classified.\n",
    "\n",
    "After hand-classifying about 5000 of the images, I decided it would be better to finish the code implementation first before focusing on classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "625ddb12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove all existing image copies from img_dir\n",
    "if os.path.exists(img_dir):\n",
    "    os.system(f'Remove-Item {img_dir}/*')\n",
    "else:\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "# copy all classified images into img_dir\n",
    "labels = pd.read_csv(labels_path, header=None)\n",
    "\n",
    "for img_filename in labels[0]:\n",
    "    # copy file from all_images_dir to img_dir\n",
    "    source = os.path.join(all_images_dir, img_filename)\n",
    "    destination = os.path.join(img_dir, img_filename)\n",
    "    shutil.copy(source, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb17507",
   "metadata": {},
   "source": [
    "### Load image characteristics\n",
    "\n",
    "While the image dataset did not come with the labels that I intended to use, I decided it would still benefit the algorithm to have access to other features that **did** come with the data. \n",
    "\n",
    "The two features that are available are what type of clothing is pictured and whether or not the clothing is for a child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf195896",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothing_characteristics = pd.read_csv('data/archives/archive/images.csv')\n",
    "\n",
    "# clean and format image data\n",
    "clothing_characteristics.sort_values(by=['image'], inplace=True)\n",
    "clothing_characteristics.reset_index(inplace=True)\n",
    "clothing_characteristics.drop(columns=['index', 'sender_id'], inplace=True)\n",
    "\n",
    "# rename columns\n",
    "clothing_characteristics.columns = ['image', 'type', 'kids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326afcd0",
   "metadata": {},
   "source": [
    "### Clean data & one-hot encode type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a7629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = clothing_characteristics['type'].copy()\n",
    "\n",
    "# create matrix of zeros, size: (rows x types)\n",
    "encoded = np.zeros((len(types), len(types.unique())))\n",
    "\n",
    "# set index at type of clothing to 1, all others 0\n",
    "# this loop takes a few seconds\n",
    "for idx in range(len(types)):\n",
    "    encoded[idx][list(types.unique()).index(types[idx])] = 1\n",
    "\n",
    "# concatenate the encoded features\n",
    "clothing_characteristics = pd.concat([clothing_characteristics, pd.DataFrame(encoded)], axis=1)\n",
    "\n",
    "# rename the columns\n",
    "clothing_characteristics.columns = np.concatenate((clothing_characteristics.columns[:3], types.unique()))\n",
    "\n",
    "# ensure all columns use underscores instead of spaces and dashes\n",
    "clothing_characteristics.columns = [c.lower().replace('-', '_').replace(' ', '_') for c in clothing_characteristics.columns]\n",
    "\n",
    "# remove the original type column\n",
    "clothing_characteristics.drop(columns=['type'], inplace=True)\n",
    "\n",
    "# convert the kids column to 1 or 0\n",
    "clothing_characteristics['kids'] = clothing_characteristics['kids'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba77c5f",
   "metadata": {},
   "source": [
    "### Creating custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b46b9dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file, header=None)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx,0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc75b0",
   "metadata": {},
   "source": [
    "### Loading images into custom PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10c6714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a custom dataset\n",
    "dataset_cnn = CustomImageDataset(labels_path, img_dir, transform_seq)\n",
    "\n",
    "# Split up the data into testing and training data\n",
    "dataset_size_cnn = dataset_cnn.__len__()\n",
    "training_size_cnn = math.floor(dataset_size_cnn * training_ratio)\n",
    "test_size_cnn = dataset_size_cnn - training_size_cnn\n",
    "\n",
    "training_data_cnn, test_data_cnn = torch.utils.data.random_split(dataset_cnn, [training_size_cnn, test_size_cnn])\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader_cnn = DataLoader(training_data_cnn, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "test_dataloader_cnn = DataLoader(test_data_cnn, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "\n",
    "all_dataloader_cnn = DataLoader(dataset_cnn, batch_size=batch_size, shuffle=True, num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8437d4",
   "metadata": {},
   "source": [
    "## Loading DCGAN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28b29490",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_gan = dset.ImageFolder(root='data/data_gan',transform=transform_seq)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader_gan = torch.utils.data.DataLoader(dataset_gan, batch_size=batch_size, shuffle=True, num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4e89f",
   "metadata": {},
   "source": [
    "## DCGAN Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dc94ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ba0993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09f475d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(ngpu).to(device)\n",
    "    \n",
    "netG.apply(weights_init);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7b7fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d65f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "netD.apply(weights_init);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02dadb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750443d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print('Starting training loop...')\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, data in enumerate(dataloader_gan, 0):\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f' % (epoch, num_epochs, i, len(dataloader_gan),errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        if (iters % 100 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader_gan)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "464d45cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[0/5][0/36]\tLoss_D: 1.6068\tLoss_G: 5.7027\tD(x): 0.7292\tD(G(z)): 0.6285 / 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x000001FB4347D080>\n",
      "Traceback (most recent call last):\n",
      "  File \"Y:\\Users\\Finn St. John\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"Y:\\Users\\Finn St. John\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"Y:\\Users\\Finn St. John\\Programs\\Python\\Python311\\Lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"Y:\\Users\\Finn St. John\\Programs\\Python\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py\", line 109, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'G_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 38\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m][\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss_D: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss_G: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mD(x): \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mD(G(z)): \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     34\u001b[0m           \u001b[38;5;241m%\u001b[39m (epoch, num_epochs, i, \u001b[38;5;28mlen\u001b[39m(dataloader_gan),\n\u001b[0;32m     35\u001b[0m              errD\u001b[38;5;241m.\u001b[39mitem(), errG\u001b[38;5;241m.\u001b[39mitem(), D_x, D_G_z1, D_G_z2))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Save Losses for plotting later\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mG_losses\u001b[49m\u001b[38;5;241m.\u001b[39mappend(errG\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     39\u001b[0m D_losses\u001b[38;5;241m.\u001b[39mappend(errD\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Check how the generator is doing by saving G's output on fixed_noise\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'G_losses' is not defined"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader_gan, 0):\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08f068",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[[1] Optimal ratio for data splitting](https://onlinelibrary.wiley.com/doi/full/10.1002/sam.11583)\n",
    "\n",
    "[[2] DCGAN Tutorial - PyTorch](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n",
    "\n",
    "[[3] Implementing Generative Adversarial Networks (GANs) for Increasing a Convolutional Neural Network’s (CNN) Performance](https://12ft.io/proxy?q=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-generative-adversarial-networks-gans-for-increasing-a-convolutional-neural-networks-f871e17fe271)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
