{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db8e4ee",
   "metadata": {},
   "source": [
    "# COGS 181 Neural Networks & Deep Learning Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3797f6",
   "metadata": {},
   "source": [
    "Import the necessary packages\n",
    "\n",
    "- Pandas and NumPy for data management\n",
    "- Matplotlib for plotting and displaying images\n",
    "- Torch and its submodules for building the network\n",
    "- Torchvision for doing image processing operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28f40ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383fcf74",
   "metadata": {},
   "source": [
    "Define the device on which we should train the network and store each tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a15c4f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "625ddb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = pd.read_csv('./classification/output/images.csv', header=None)\n",
    "image_data = pd.read_csv('./archive/images.csv')\n",
    "\n",
    "# clean and format classifications\n",
    "classifications.columns = ['image', 'class']\n",
    "classifications['image'] = classifications['image'].apply(lambda x: '.'.join(x.split('.')[:-1]))\n",
    "\n",
    "# clean and format image data\n",
    "image_data.sort_values(by=['image'], inplace=True)\n",
    "image_data.reset_index(inplace=True)\n",
    "image_data.drop(columns=['index', 'sender_id'], inplace=True)\n",
    "\n",
    "image_data.columns = ['image', 'type', 'kids']\n",
    "\n",
    "types = image_data['type']\n",
    "\n",
    "encoded = np.zeros((len(types), len(types.unique())))\n",
    "\n",
    "for idx in range(len(types)):\n",
    "    encoded[idx][list(types.unique()).index(types[idx])] = 1\n",
    "    \n",
    "image_data = pd.concat([image_data, pd.DataFrame(encoded)], axis=1)\n",
    "\n",
    "image_data.columns = np.concatenate((image_data.columns[:3], types.unique()))\n",
    "\n",
    "image_data.columns = [c.lower().replace('-', '_').replace(' ', '_') for c in image_data.columns]\n",
    "\n",
    "image_data.drop(columns=['type'], inplace=True)\n",
    "\n",
    "image_data['kids'] = image_data['kids'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b01a55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images, takes about a minute\n",
    "\n",
    "PATH = './archive/images_compressed'\n",
    "\n",
    "image_paths = np.array([f'{PATH}/{i}' for i in os.listdir(PATH)])\n",
    "\n",
    "image_paths.shape\n",
    "\n",
    "tensor_images = []\n",
    "\n",
    "for path in image_paths:\n",
    "    try:\n",
    "        tensor_images.append(torchvision.io.read_image(path))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ff4bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(tsr_img):\n",
    "    plt.imshow(tsr_img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3028bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_seq = transforms.Compose([\n",
    "    transforms.Resize(size=(200, 200), antialias=True),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e662a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in tensor_images:\n",
    "    img.to(device)\n",
    "    img.type(dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0d72d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx,0])\n",
    "        image = torchvision.read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd0a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
